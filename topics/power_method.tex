\documentclass[12pt]{article}
\usepackage{amsmath, tcolorbox}
\bibliographystyle{plain} 
\begin{document}

Let me emphasize main points of the power methods described in \cite{borm2012numerical}.
Of course, I will add some points of myself to make some concepts more clear.


\section{Baisc power methods}
After establishing $A^{k}x$ converges to the eigenvector of $A$ corresponding to the largest eigenvalue, 
we need some stopping criterion, which is the main concern in numerical analysis because we cannot set max\textunderscore eiter simply. 

Since $x^{(m)}$ cannot be eigenvector identically, there is no real number $a$ satisfying $Ax^{(m)} = ax^{(m)}$. 
But we can expect the number $a$ which makes
$$
\|A x^{(m)}-ax\| 
$$
minimizes is a good approximation of the largest eigenvalue. Now we elaborate on how to solve this $a$.

\paragraph{Rayleigh Quotient.} If one see $Av = av$ as an overdetermined equation system, technique about \textbf{least square} can be applied
to get the solution $a$ which is called Rayleigh quotient.

\begin{equation}
    \label{RayleighQ}
    \Lambda_{A}(v) := \frac{v^{*}Av}{v^{*}v}
\end{equation}

Geometrically, the solution of least square is just to find a scalar $a$ such that $Av-av$ is orthogonal to $v$.

\begin{tcolorbox}[colback=gray!10, colframe=black, title=Rayleigh quotient]
    One can also deduce the same result \eqref{RayleighQ} algebraically by solving the following optimization problem
    \begin{equation}
        \label{optimization1}
        \operatorname{min}_{x} \|Av-a v\|_{2}^{2},
    \end{equation}
    then eq \eqref{optimization1} (take real case as example) boils down to finding the critical point of
    $$
    f(a)=v^{T}v a^{2}- v^{T}(A^{T}+A)v a+v^{T}A^{T}Av
    $$
    If $v^{T}A^{T}v$ is a real number, then $v^{T}A^{T}v = v^{T}Av$, one can use the derivative of $f(a)$ to determine the critical point and the result matches the Least Square. 
    When we consider the complex number field, the expansion of $f(x)=\operatorname{min}_{x} \|Av-a v\|_{2}^{2}$ is subtle, saying
    $$
    f(a)\neq v^{*}v a^{2}- v^{*}(A^{*}+A)v a+v^{*}A^{*}Av
    $$
    especially when $a$ is complex.
    \end{tcolorbox}




The theoretical support is given by Theorem 4.6 in \cite{borm2012numerical}.

Combined with $\|x^{(m)}-v\|\approx 0$ and $|\lambda^{(m)} -\lambda|\approx 0$ where $\lambda, v\in\operatorname{eig}(A)$, we can see that
$$
\|Ax^{(m)}-\lambda^{(m)}x^{(m)}\| \to 0
$$
where each components can be computed explicitly in the process of iteration. So one option for stopping criterion is to set $\epsilon$ and stop the iteration when
$$
\|Ax^{(m)}-\lambda^{(m)x^{(m)}}\| < \epsilon
$$

\section{Simultaneous Iteration}

Suppose we want to find the first $k$ eigenvectors of $A$, we can use the simultaneous iteration. 
We expect $V^{(m+1)} = A^{m}V$ where $V$ is matrix consisting of $k$ independent column vectors to converge to the eigenvectors of $A$ corresponding to the largest $k$ eigenvalues.
Although we can prove that $V^{(m+1)}$ is injective but it's just theoretical! In practice, we need to consider the numerical stability of the algorithm. Let me give an example to show the problem

Consider the symmetric matrix:

$$
A = \begin{bmatrix} 4 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 2 \end{bmatrix}
$$

We will start with the matrix $V$ with two columns in the initial step:

$$
V^{(0)} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}
$$

\subsubsection*{Iteration Without Orthogonalization}

Each column of $ V^{(m)} $ is updated via:
$$
V^{(m+1)} = A V^{(m)}
$$

Perform a few iterations to illustrate the convergence:

\paragraph{First Iteration:}

$$
V^{(1)} = A V^{(0)} = \begin{bmatrix} 4 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 2 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 4 & 1 \\ 1 & 3 \\ 1 & 1 \end{bmatrix}
$$

\paragraph{Second Iteration:}

$$
V^{(2)} = A V^{(1)} = \begin{bmatrix} 4 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 2 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 3 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 18 & 8 \\ 8 & 11 \\ 7 & 6 \end{bmatrix}
$$

\paragraph{... 10th Iteration:}

$$
V^{(10)} = A V^{(9)} = \begin{bmatrix} 8490718 & 5843148 \\ 5843148 & 4032371 \\ 4458347 & 3073546 \end{bmatrix}
$$

\subsubsection*{Observation}

As iterations continue, each column increasingly aligns with the dominant eigenvector $\mathbf{v}_1 = \begin{bmatrix} 0.7558 \\ 0.5207 \\ 0.3971 \end{bmatrix}$ corresponding to $\lambda_{\max} = 5.2143$. For large $ m $, the columns of $ V^{(m)} $ converge to:

$$
V^{(m)} \approx \begin{bmatrix} 0.7558 & 0.5207 \\ 0.3971 & 0.7558 \\ 0.5207 & 0.3971 \end{bmatrix}
$$

This convergence indicates numerical instability, as the columns lose linear independence and all vectors converge toward the dominant eigenvector's direction.

\subsubsection*{Apply QR into Simultaneous Iteration}

\paragraph{Orthogonalize (using QR Factorization):}

Perform QR factorization when computing $V^{(2)}= AV^{(1)} = A$ where
$$
V^{(1)} = \begin{bmatrix} 4 & 1 \\ 1 & 3 \\ 1 & 1 \end{bmatrix} =Q^{(1)}R^{(1)}=  \begin{bmatrix}
-0.9428 & 0.2851 \\
-0.2357 & -0.9366 \\
-0.2357 & -0.2036
\end{bmatrix} \begin{bmatrix}
-4.2426 & -1.8856 \\
0 & -2.7285
\end{bmatrix}
$$,
then we compute the QR of $AQ^{(1)} = Q^{2} \hat{R}^{(2)}$,

$$
\begin{aligned}
    V^{(2)}& =Q^{(2)} \hat{R}^{(2)}R^{(1)}\\
            & = \begin{bmatrix}
-0.8611 & 0.4685 \\
-0.3827 & -0.8531 \\
-0.3349 & -0.2297
\end{bmatrix} \begin{bmatrix}
4.9272 & 1.3987 \\
0 & 2.5708
\end{bmatrix}\begin{bmatrix}
-4.2426 & -1.8856 \\
0 & -2.7285
\end{bmatrix}
\end{aligned}
$$

\paragraph{Update $V^{(m)}$ with orthogonal columns:} 

After orthogonalization:
$$
V^{(m)}=Q^{(m)} R^{(m)} =\begin{pmatrix}
-0.7559 & 0.6316 \\
-0.5205 & -0.7395 \\
-0.3971 & -0.2330
\end{pmatrix} 1.0e+07 \begin{pmatrix}
-5.8556 & -4.0339 \\
0 & -0.0024
\end{pmatrix}
$$
with $m = 10$ as example. One can verify that the columns of $Q^{(10)}$ closely approximate the eigenvectors of $A$ corresponding to its two largest eigenvalues (in magnitude).

This orthogonalization corrects the numerical errors that lead to loss of linear independence, thus ensuring each iteration step mainly jumps to the invariant subspace.

\bibliography{reference.bib} % The name of your .bib file without the .bib extension
\end{document}